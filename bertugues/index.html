<!DOCTYPE html>
<html lang="pt-br">

<head>
  <title>
  BERTugues ¬∑ Ricardo Zago
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">


<meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests; block-all-mixed-content; default-src 'self'; child-src 'self'; font-src 'self' https://fonts.gstatic.com https://cdn.jsdelivr.net/; form-action 'self'; frame-src 'self'; img-src 'self'; object-src 'none'; style-src 'self' 'unsafe-inline' https://fonts.googleapis.com/ https://cdn.jsdelivr.net/; script-src 'self' 'unsafe-inline' https://www.google-analytics.com https://cdn.jsdelivr.net/; connect-src 'self' https://www.google-analytics.com;">




<meta name="author" content="Ricardo Zago">
<meta name="description" content="O BERTugues √© um modelo de embedding de texto para o portugu√™s Brasileiro treinato com dados exclusivamente em portugu√™s.
Use o BERTugues! Acesse pelo Hugging Face!

O BERTugues foi pr√©-treinado seguindo os mesmos passos do paper original do BERT, com os objetivos de Masked Language Modeling (MLM) e Next Sentence Prediction (NSP), com 1 milh√£o de steps, usando mais de 20 GB de textos. Como o Bertimbau, foi pr√©-treinado com o dataset BrWAC e a Wikip√©dia em portugu√™s para o Tokenizador, contando com algumas melhorias no fluxo de treinamento, como:">
<meta name="keywords" content="blog,desenvolvedor,pessoal">



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="BERTugues">
  <meta name="twitter:description" content="O BERTugues √© um modelo de embedding de texto para o portugu√™s Brasileiro treinato com dados exclusivamente em portugu√™s.
Use o BERTugues! Acesse pelo Hugging Face!
O BERTugues foi pr√©-treinado seguindo os mesmos passos do paper original do BERT, com os objetivos de Masked Language Modeling (MLM) e Next Sentence Prediction (NSP), com 1 milh√£o de steps, usando mais de 20 GB de textos. Como o Bertimbau, foi pr√©-treinado com o dataset BrWAC e a Wikip√©dia em portugu√™s para o Tokenizador, contando com algumas melhorias no fluxo de treinamento, como:">

<meta property="og:url" content="https://ricardozago.github.io/bertugues/">
  <meta property="og:site_name" content="Ricardo Zago">
  <meta property="og:title" content="BERTugues">
  <meta property="og:description" content="O BERTugues √© um modelo de embedding de texto para o portugu√™s Brasileiro treinato com dados exclusivamente em portugu√™s.
Use o BERTugues! Acesse pelo Hugging Face!
O BERTugues foi pr√©-treinado seguindo os mesmos passos do paper original do BERT, com os objetivos de Masked Language Modeling (MLM) e Next Sentence Prediction (NSP), com 1 milh√£o de steps, usando mais de 20 GB de textos. Como o Bertimbau, foi pr√©-treinado com o dataset BrWAC e a Wikip√©dia em portugu√™s para o Tokenizador, contando com algumas melhorias no fluxo de treinamento, como:">
  <meta property="og:locale" content="pt_br">
  <meta property="og:type" content="article">




<link rel="canonical" href="https://ricardozago.github.io/bertugues/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.7763f8bc6341ecf82378e867c285e1549abb063a899be313ccd25dbfcd24fa7d.css" integrity="sha256-d2P4vGNB7PgjeOhnwoXhVJq7BjqJm&#43;MTzNJdv80k&#43;n0=" crossorigin="anonymous" media="screen" />








 




<link rel="icon" type="image/svg+xml" href="/images/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>




<body class="preload-transitions colorscheme-light">
  

  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="https://ricardozago.github.io/">
      Ricardo Zago
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/bertugues/">BERTugues</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/disciplinas/datascience/">Lectures</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/publicacoes/">Publications</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/about/">About</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container page">
  <article>
    <header>
      <h1 class="title">
        <a class="title-link" href="https://ricardozago.github.io/bertugues/">
          BERTugues
        </a>
      </h1>
    </header>

    <p>O BERTugues √© um modelo de embedding de texto para o portugu√™s Brasileiro treinato com dados exclusivamente em portugu√™s.</p>
<p><a href="https://huggingface.co/ricardoz/BERTugues-base-portuguese-cased"  class="external-link" target="_blank" rel="noopener">Use o BERTugues! Acesse pelo Hugging Face!</a></p>
<p><img src="Hugginface.png" alt="huggingface"></p>
<p>O BERTugues foi pr√©-treinado seguindo os mesmos passos do <a href="https://arxiv.org/abs/1810.04805v2"  class="external-link" target="_blank" rel="noopener">paper original do BERT</a>, com os objetivos de Masked Language Modeling (MLM) e Next Sentence Prediction (NSP), com 1 milh√£o de <em>steps</em>, usando mais de 20 GB de textos. Como o <a href="https://huggingface.co/neuralmind/bert-base-portuguese-cased"  class="external-link" target="_blank" rel="noopener">Bertimbau</a>, foi pr√©-treinado com o dataset <a href="https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC"  class="external-link" target="_blank" rel="noopener">BrWAC</a> e a Wikip√©dia em portugu√™s para o Tokenizador, contando com algumas melhorias no fluxo de treinamento, como:</p>
<ul>
<li><strong>Remo√ß√£o de caracteres pouco comuns no portugu√™s do treinamento do Tokenizador.</strong> No Bertimbau mais de 7000 dos 29794 usam caracteres orientais ou especiais, quase nunca usados no portugu√™s. Por exemplo, existem os tokens: &ldquo;##Êº´&rdquo;, &ldquo;##Ïºù&rdquo;, &ldquo;##Ââç&rdquo;, j√° no BERTugues fizemos a remo√ß√£o desses caracteres antes de treinar o tokenizador;</li>
<li><strong>üòÄ Adi√ß√£o dos principais Emojis no Tokenizador.</strong> A Wikip√©dia conta com poucos Emojis em seu texto, dessa forma um baixo n√∫mero de Emojis entravam nos Tokens. Como j√° <a href="https://arxiv.org/abs/1910.13793"  class="external-link" target="_blank" rel="noopener">demonstrado na literatura</a>, eles s√£o importantes numa s√©rie de tarefas;</li>
<li><strong>Filtragem de qualidade dos textos do BrWAC</strong> seguindo o modelo heur√≠stica proposta pelo <a href="https://arxiv.org/abs/2112.11446"  class="external-link" target="_blank" rel="noopener">paper do modelo Gopher</a> do Google, onde removemos do BrWac textos de baixa qualidade.</li>
</ul>
<h2 id="tokenizador">
  Tokenizador
  <a class="heading-link" href="#tokenizador">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link para o cabe√ßalho"></i>
    <span class="sr-only">Link para o cabe√ßalho</span>
  </a>
</h2>
<p>Com a substitui√ß√£o dos Tokens pouco usados no portugu√™s conseguimos diminuir a quantidade m√©dia de palavras que s√£o quebradas em mais de um Token. No teste utilizando o <a href="https://huggingface.co/datasets/assin2"  class="external-link" target="_blank" rel="noopener">assin2</a>, mesmo dataset utilizando pelo Bertimbau para fazer o teste na disserta√ß√£o de mestrado, diminu√≠mos a quantidade m√©dia de palavras quebradas por texto de 3,8 para 3,0, no BERT multilinguagem esse n√∫mero era 7,4.</p>
<p><img src="subtokens_counts.png" alt="subtokens"></p>
<h2 id="performance">
  Performance
  <a class="heading-link" href="#performance">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link para o cabe√ßalho"></i>
    <span class="sr-only">Link para o cabe√ßalho</span>
  </a>
</h2>
<p>Para comparar a performance testamos um problema de classifica√ß√£o de textos, utilizando a base de reviews de filmes do <a href="https://www.kaggle.com/datasets/luisfredgs/imdb-ptbr"  class="external-link" target="_blank" rel="noopener">IMDB</a>, que foi traduzida para o portugu√™s e possui uma boa qualidade. Neste problema utilizamos a representa√ß√£o do BERTugues da frase e passamos ela por um modelo de Random Forest para realizar a classifica√ß√£o.</p>
<p>Tamb√©m utilizamos a compara√ß√£o de performance do paper <a href="https://repositorio.ufms.br/handle/123456789/5119"  class="external-link" target="_blank" rel="noopener">JurisBERT: Transformer-based model for embedding legal texts</a>, que pr√©-treina um BERT especialmente para textos de um dom√≠nio, usando o BERT multilinguagem e o Bertimbau como baseline. Neste caso utilizamos o <a href="https://github.com/alfaneo-ai/brazilian-legal-text-benchmark"  class="external-link" target="_blank" rel="noopener">c√≥digo disponibilizado pelo time do paper</a> e adicionamos o BERTugues. O modelo √© utilizado para comparar se dois textos s√£o do mesmo assunto ou n√£o.</p>
<table>
  <thead>
      <tr>
          <th>Modelo</th>
          <th>IMDB (F1)</th>
          <th>STJ (F1)</th>
          <th>PJERJ (F1)</th>
          <th>TJMS (F1)</th>
          <th>M√©dia F1</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>BERT Multilinguagem</td>
          <td>72,0%</td>
          <td>30,4%</td>
          <td>63,8%</td>
          <td>65,0%</td>
          <td>57,8%</td>
      </tr>
      <tr>
          <td>Bertimbau-Base</td>
          <td>82,2%</td>
          <td>35,6%</td>
          <td>63,9%</td>
          <td>71,2%</td>
          <td>63,2%</td>
      </tr>
      <tr>
          <td>Bertimbau-Large</td>
          <td><strong>85,3%</strong></td>
          <td>43,0%</td>
          <td>63,8%</td>
          <td><strong>74,0%</strong></td>
          <td>66,5%</td>
      </tr>
      <tr>
          <td>BERTugues-Base</td>
          <td>84,0%</td>
          <td><strong>45,2%</strong></td>
          <td><strong>67,5%</strong></td>
          <td>70,0%</td>
          <td><strong>66,7%</strong></td>
      </tr>
  </tbody>
</table>
<p>O BERTugues foi superior em 3 de 4 tarefa em rela√ß√£o ao Bertimbau-base e em 2 das 4 tarefas superior ao Bertimbau-Large, um modelo muito maior (3x mais par√¢metros) e custoso computacionalmente.</p>
<h2 id="exemplo-de-uso">
  Exemplo de uso
  <a class="heading-link" href="#exemplo-de-uso">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link para o cabe√ßalho"></i>
    <span class="sr-only">Link para o cabe√ßalho</span>
  </a>
</h2>
<p>Diversos exemplos de uso est√£o dispon√≠veis no nosso  <a href="https://github.com/ricardozago/BERTugues"  class="external-link" target="_blank" rel="noopener">nosso Github</a>. Para uma r√°pida consulta adicionamos 2 exemplos abaixo:</p>
<p>Predi√ß√£o de palavras mascaradas (Masked Language Modeling):</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="font-weight:bold">transformers</span> <span style="font-weight:bold">import</span> BertTokenizer, BertForMaskedLM, pipeline
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model = BertForMaskedLM.from_pretrained(<span style="font-style:italic">&#34;ricardoz/BERTugues-base-portuguese-cased&#34;</span>)
</span></span><span style="display:flex;"><span>tokenizer = BertTokenizer.from_pretrained(<span style="font-style:italic">&#34;ricardoz/BERTugues-base-portuguese-cased&#34;</span>, do_lower_case=<span style="font-weight:bold">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pipe = pipeline(<span style="font-style:italic">&#39;fill-mask&#39;</span>, model=model, tokenizer=tokenizer, top_k = 3)
</span></span><span style="display:flex;"><span>pipe(<span style="font-style:italic">&#39;[CLS] Eduardo abriu os [MASK], mas n√£o quis se levantar. Ficou deitado e viu que horas eram.&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="font-style:italic"># [{&#39;score&#39;: 0.7272418141365051,</span>
</span></span><span style="display:flex;"><span><span style="font-style:italic">#   &#39;token&#39;: 7292,</span>
</span></span><span style="display:flex;"><span><span style="font-style:italic">#   &#39;token_str&#39;: &#39;olhos&#39;,</span>
</span></span><span style="display:flex;"><span><span style="font-style:italic">#   &#39;sequence&#39;: &#39;Eduardo abriu os olhos, mas n√£o quis se levantar. Ficou deitado e viu que horas eram.&#39;},</span>
</span></span><span style="display:flex;"><span><span style="font-style:italic">#  {&#39;score&#39;: 0.2677205801010132,</span>
</span></span><span style="display:flex;"><span><span style="font-style:italic">#   &#39;token&#39;: 12761,</span>
</span></span><span style="display:flex;"><span><span style="font-style:italic">#   &#39;token_str&#39;: &#39;bra√ßos&#39;,</span>
</span></span><span style="display:flex;"><span><span style="font-style:italic">#   &#39;sequence&#39;: &#39;Eduardo abriu os bra√ßos, mas n√£o quis se levantar. Ficou deitado e viu que horas eram.&#39;},</span>
</span></span><span style="display:flex;"><span><span style="font-style:italic">#  {&#39;score&#39;: 0.0007434834260493517,</span>
</span></span><span style="display:flex;"><span><span style="font-style:italic">#   &#39;token&#39;: 24298,</span>
</span></span><span style="display:flex;"><span><span style="font-style:italic">#   &#39;token_str&#39;: &#39;√≥culos&#39;,</span>
</span></span><span style="display:flex;"><span><span style="font-style:italic">#   &#39;sequence&#39;: &#39;Eduardo abriu os √≥culos, mas n√£o quis se levantar. Ficou deitado e viu que horas eram.&#39;}]</span>
</span></span></code></pre></div><p>Cria√ß√£o de um embedding para uma frase:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="font-weight:bold">transformers</span> <span style="font-weight:bold">import</span> BertTokenizer, BertModel, pipeline
</span></span><span style="display:flex;"><span><span style="font-weight:bold">import</span> <span style="font-weight:bold">torch</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model = BertModel.from_pretrained(<span style="font-style:italic">&#34;ricardoz/BERTugues-base-portuguese-cased&#34;</span>)
</span></span><span style="display:flex;"><span>tokenizer = BertTokenizer.from_pretrained(<span style="font-style:italic">&#34;ricardoz/BERTugues-base-portuguese-cased&#34;</span>, do_lower_case=<span style="font-weight:bold">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>input_ids = tokenizer.encode(<span style="font-style:italic">&#39;[CLS] Eduardo abriu os olhos, mas n√£o quis se levantar. Ficou deitado e viu que horas eram.&#39;</span>, return_tensors=<span style="font-style:italic">&#39;pt&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="font-weight:bold">with</span> torch.no_grad():
</span></span><span style="display:flex;"><span>    last_hidden_state = model(input_ids).last_hidden_state[:, 0]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>last_hidden_state
</span></span><span style="display:flex;"><span>	
</span></span><span style="display:flex;"><span><span style="font-style:italic"># tensor([[-1.5727e+00,  5.4707e-01, -2.6169e-01, -3.0714e-01, -9.4545e-01,</span>
</span></span><span style="display:flex;"><span><span style="font-style:italic">#           4.1557e-01, -7.9884e-01, -2.1481e-01,  5.9792e-01, -1.4198e+00,</span>
</span></span><span style="display:flex;"><span><span style="font-style:italic">#           1.1929e-01,  8.0228e-01,  5.4977e-01,  1.2710e+00, -2.9444e-01,</span>
</span></span><span style="display:flex;"><span><span style="font-style:italic">#           1.3398e+00,  1.4413e-01,  8.2983e-01, -8.2349e-02,  1.8593e-01,</span>
</span></span></code></pre></div><h2 id="mais-informa√ß√µes">
  Mais informa√ß√µes
  <a class="heading-link" href="#mais-informa%c3%a7%c3%b5es">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link para o cabe√ßalho"></i>
    <span class="sr-only">Link para o cabe√ßalho</span>
  </a>
</h2>
<p>Para mais informa√ß√µes acesso <a href="https://github.com/ricardozago/BERTugues"  class="external-link" target="_blank" rel="noopener">nosso Github</a>!</p>

  </article>
</section>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css"
    integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
  
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"
    integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body,
      {
        delimiters: [
          {left: '$$', right: '$$', display:true},
          {left: '$', right: '$', display:false},
          {left: '\\(', right: '\\)', display: false},
          {left: '\\[', right: '\\]', display: true}
        ]
      }
    );"></script>

    </div>

    <footer class="footer">
  <section class="container">
    ¬©
    
    2025
     Ricardo Zago 
    ¬∑
    
    Promovido por <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js" integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>
</html>
